{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Bias in Hate Speech Detection – Von einfachen Modellen zu Counterfactual Fairness\n","\n","Dieses Notebook kombiniert Elemente aus zwei Tutorials:\n","\n","- https://www.kaggle.com/alexisbcook/identifying-bias-in-ai\n","- https://github.com/biaslyze-dev/biaslyze\n","\n","Ziel ist es, die Entstehung und Analyse von Bias in Hate Speech Detection Modellen nachzuvollziehen.\n","Dazu starten wir mit einem sehr einfachen Modell und steigern schrittweise die Komplexität, bis wir schließlich systematisch Counterfactual Fairness messen."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.014195,"end_time":"2020-10-30T19:06:15.040067","exception":false,"start_time":"2020-10-30T19:06:15.025872","status":"completed"},"tags":[]},"source":["# 1. Einführung: Bias in Toxicity Detection\n","\n","Am Ende des Jahres 2017 stellte die Plattform Civil Comments den Betrieb ein und veröffentlichte ca. 2 Millionen moderierte Kommentare. Jigsaw unterstützte die Annotation der Daten, und 2019 organisierte Kaggle die Challenge \"Unintended Bias in Toxicity Classification\"\n","\n","Diese Daten eignen sich gut, um zu untersuchen, wie Modelle für Hate Speech Detection Verzerrungen entwickeln können - oft ohne, dass dies beabsichtigt ist.\n","Wir beginnen mit einem stark vereinfachten Setup, um grundlegende Muster sichtbar zu machen.\n","\n","The code cell below loads some of the data from the competition.  We'll work with thousands of comments, where each comment is labeled as either \"toxic\" or \"not toxic\".\n","\n","Begin by running the next code cell.  \n","- Clicking inside the code cell.\n","- Click on the triangle (in the shape of a \"Play button\") that appears to the left of the code cell.\n","\n","The code will run for approximately 30 seconds.  When it finishes, you should see as output a message saying that the data was successfully loaded, along with two examples of comments: one is toxic, and the other is not."]},{"cell_type":"markdown","metadata":{},"source":["# 1.2 Einstieg: Einfaches Modell basierend auf Worthäufigkeiten\n","\n","Wir laden die Daten, wählen ein Subset und repräsentieren die Texte über reine Wortzählungen. Durch diese bewusst simple Repräsentation werden früh erste Hinweise auf Bias sichtbar.\n","\n","Die folgenden Schritte laden und vektorisieren die Daten und trainieren ein logistisches Regressionsmodell."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODO logistisches Regressionsmodell"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:07:52.668284Z","iopub.status.busy":"2025-11-05T16:07:52.667946Z","iopub.status.idle":"2025-11-05T16:08:05.433279Z","shell.execute_reply":"2025-11-05T16:08:05.432024Z","shell.execute_reply.started":"2025-11-05T16:07:52.668252Z"},"papermill":{"duration":30.692706,"end_time":"2020-10-30T19:06:45.747191","exception":false,"start_time":"2020-10-30T19:06:15.054485","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","import matplotlib.pyplot as plt\n","\n","# Get the same results each time\n","np.random.seed(0)\n","\n","# Load the training data\n","data = pd.read_csv(\"data.csv\")\n","comments = data[\"comment_text\"]\n","target = (data[\"target\"]>0.7).astype(int)\n","\n","# Break into training and test sets\n","comments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n","\n","# Get vocabulary from training data\n","vectorizer = CountVectorizer()\n","vectorizer.fit(comments_train)\n","\n","# Get word counts for training and test sets\n","X_train = vectorizer.transform(comments_train)\n","X_test = vectorizer.transform(comments_test)\n","\n","# Preview the dataset\n","print(\"Data successfully loaded!\\n\")\n","print(\"Sample toxic comment:\", comments_train.iloc[22])\n","print(\"Sample not-toxic comment:\", comments_train.iloc[17])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# first 10 entries of the dataset\n","data[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:22:00.060313Z","iopub.status.busy":"2025-11-05T16:22:00.060003Z","iopub.status.idle":"2025-11-05T16:22:00.076191Z","shell.execute_reply":"2025-11-05T16:22:00.074359Z","shell.execute_reply.started":"2025-11-05T16:22:00.060290Z"},"trusted":true},"outputs":[],"source":["# Take a look at the dataset, e.g how many samples are there that contain the word 'black' that are labled as toxic\n","\n","len(data.loc[(data['comment_text'].str.contains(\"black\", case=False, na=False)) & (data['target'] >= 0.7)])"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.012727,"end_time":"2020-10-30T19:06:45.774334","exception":false,"start_time":"2020-10-30T19:06:45.761607","status":"completed"},"tags":[]},"source":["# 1.3 Modelltraining und erste Evaluation\n","\n","Das folgende Code-Snippet trainiert ein sehr einfaches Modell. Es verwendet CountVectorizer (zählt Häufigkeit von Wörtern im comment_text) und Logistic Regression. Anschließend zeigt es die Testgenauigkeit."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:08:49.992604Z","iopub.status.busy":"2025-11-05T16:08:49.992264Z","iopub.status.idle":"2025-11-05T16:09:06.603838Z","shell.execute_reply":"2025-11-05T16:09:06.602824Z","shell.execute_reply.started":"2025-11-05T16:08:49.992578Z"},"papermill":{"duration":11.047454,"end_time":"2020-10-30T19:06:56.836704","exception":false,"start_time":"2020-10-30T19:06:45.78925","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","import joblib\n","\n","# Train a model and evaluate performance on test dataset\n","#classifier = LogisticRegression(max_iter=2000)\n","#classifier.fit(X_train, y_train)\n","\n","# load already trained model\n","classifier = joblib.load('logistic_model_vectorizer.pkl')\n","score = classifier.score(X_test, y_test)\n","print(\"Accuracy:\", score)\n","\n","# Function to classify any string\n","def classify_string(string, investigate=False):\n","    prediction = classifier.predict(vectorizer.transform([string]))[0]\n","    if prediction == 0:\n","        print(\"NOT TOXIC:\", string)\n","    else:\n","        print(\"TOXIC:\", string)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.014849,"end_time":"2020-10-30T19:06:56.868884","exception":false,"start_time":"2020-10-30T19:06:56.854035","status":"completed"},"tags":[]},"source":["Rund 93% Genauigkeit wirken zunächst überzeugend – doch diese Zahl sagt nichts über mögliche Ungleichbehandlung/ Diskriminierung aus.\n","\n","# 1.4 Erste Exploration: Wie reagiert das Modell auf eigene Beispiele?\n","\n","Hier kannst du eigene Kommentare testen und beobachten, wie das Modell sie einordnet.\n","\n","Die Funktion classify_string sorgt für die Ausgabe, z. B.:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:11:57.588408Z","iopub.status.busy":"2025-11-05T16:11:57.588075Z","iopub.status.idle":"2025-11-05T16:11:57.598306Z","shell.execute_reply":"2025-11-05T16:11:57.597148Z","shell.execute_reply.started":"2025-11-05T16:11:57.588385Z"},"papermill":{"duration":0.026189,"end_time":"2020-10-30T19:06:56.90901","exception":false,"start_time":"2020-10-30T19:06:56.882821","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Comment to pass through the model\n","my_comment = \"I have a black friend\"\n","\n","# Do not change the code below\n","classify_string(my_comment)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.018539,"end_time":"2020-10-30T19:06:56.942426","exception":false,"start_time":"2020-10-30T19:06:56.923887","status":"completed"},"tags":[]},"source":["# 1.5 Feature-Gewichte: Welche Wörter gelten als besonders toxisch?\n","\n","Das Modell weist jedem vorkommenden Wort einen Koeffizienten zu. Hohe Werte bedeuten „toxisch“, niedrige „nicht toxisch“.\n","\n","Die folgende Codezelle listet die 10 am stärksten positiv gewichteten Wörter:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:15:41.060948Z","iopub.status.busy":"2025-11-05T16:15:41.060580Z","iopub.status.idle":"2025-11-05T16:15:41.129253Z","shell.execute_reply":"2025-11-05T16:15:41.127661Z","shell.execute_reply.started":"2025-11-05T16:15:41.060897Z"},"papermill":{"duration":0.115908,"end_time":"2020-10-30T19:06:57.07637","exception":false,"start_time":"2020-10-30T19:06:56.960462","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\n","print(coefficients.sort_values(by=['coeff']).tail(10))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.015335,"end_time":"2020-10-30T19:06:57.106908","exception":false,"start_time":"2020-10-30T19:06:57.091573","status":"completed"},"tags":[]},"source":["Schaut euch die 'toxischsten' Wörter der Zelle oberhalb an. Seid ihr überrascht? Gibt es Wörter die nicht in der Liste sein sollten? "]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.015333,"end_time":"2020-10-30T19:06:57.175887","exception":false,"start_time":"2020-10-30T19:06:57.160554","status":"completed"},"tags":[]},"source":["# 1.6 Sensitivität gegenüber Gruppenbegriffen\n","\n","In diesem Abschnitt werden minimal veränderte Sätze eingegeben, um zu sehen, wie stark das Modell auf Begriffe reagiert wie:\n","\n","- muslim / christian\n","- white / black\n","\n","Dies ist ein klassisches Setup zur Erkennung von bias-sensitiven Token."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-11-05T16:16:42.908238Z","iopub.status.busy":"2025-11-05T16:16:42.907844Z","iopub.status.idle":"2025-11-05T16:16:42.928812Z","shell.execute_reply":"2025-11-05T16:16:42.927675Z","shell.execute_reply.started":"2025-11-05T16:16:42.908214Z"},"papermill":{"duration":0.044861,"end_time":"2020-10-30T19:06:57.236559","exception":false,"start_time":"2020-10-30T19:06:57.191698","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Set the value of new_comment\n","new_comment = \"I have a white friend\"\n","\n","# Do not change the code below\n","classify_string(new_comment)\n","coefficients[coefficients.word.isin(new_comment.split())]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.016152,"end_time":"2020-10-30T19:06:57.26916","exception":false,"start_time":"2020-10-30T19:06:57.253008","status":"completed"},"tags":[]},"source":["Es wird sichtbar, dass das Modell teilweise unterschiedliche Gewichte für ähnliche Begriffe verwendet, obwohl der Kontext derselbe ist.\n","\n","# 1.7 Reflexion: Erste Erklärung zu Bias-Effekten\n","\n","Warum passiert das?\n","\n","Ursachen können u. a. sein:\n","\n","- Ungleichverteilung sensibler Begriffe in den Trainingsdaten\n","- Häufige Korrelation zwischen Minderheitengruppen und toxischen Kontexten\n","- Einfaches zählen von Worthäufigkeiten ignoriert Kontext\n","- Einzelne Begriffe werden überinterpretiert\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Etwas komplexere Repräsentation und größere Datenbasis\n","\n","Jetzt wechseln wir auf einen größeren Datensatz und nutzen TF-IDF statt roher Frequenzen.\n","Das Ziel: Eine robustere Repräsentation, die häufige Wörter weniger stark gewichtet.\n","\n","Wir verwenden:\n","\n","- TF-IDF Vectorizer()\n","- Logistic Regression\n","- deutlich größeren Datensatz (226.235 Kommentare)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load data\n","df = pd.read_csv(\"train.csv\"); df.tail()\n","# all that is labeled as toxis, severe_toxic, obscene... gets labeled as toxic\n","df[\"target\"] = df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].sum(axis=1) > 0\n","df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Have a look at the dataset\n","\n","# filters data where both target == 1 -> labeled as toxic and the comment text inludes a certain word: here 'old'\n","len(df.loc[(df['comment_text'].str.contains(\"old\", case=False, na=False)) & (df['target'] == 1)]['comment_text'])\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2.1 Bias-Analyse mit Biaslyze\n","\n","Nun führen wir eine systematische Analyse durch.\n","Biaslyze nutzt counterfactual token fairness:\n","\n","Wie stark verändert sich die Modell-Prediktion, wenn ein sensitives Wort durch ein anderes aus derselben Kategorie ersetzt wird?\n","\n","Beispiele für Konzepte:\n","\n","- religion\n","- gender\n","- ethnicity\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train model\n","#clf = make_pipeline(TfidfVectorizer(min_df=10, max_features=30000, stop_words=\"english\"), LogisticRegression(C=10))\n","#clf.fit(df.comment_text, df.target)\n","\n","#load already trained model\n","clf = joblib.load(\"Tfidf_Regression_jigsaw.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Try some examples\n","\n","if clf.predict(['I have a gay friend'])[0] == True:\n","    print(\"TOXIC\")\n","else:\n","    print('NOT TOXIC')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from biaslyze.bias_detectors import CounterfactualBiasDetector\n","from biaslyze.concept_class import Concept\n","\n","bias_detector = CounterfactualBiasDetector()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### OPTIONAL ####\n","\n","# lets have a look at the Concepts \n","\n","'''Example Concept:  \"religion\": [\n","        {\"keyword\": \"jew\", \"function\": [\"NOUN\"], \"category\": \"judaism\"},\n","        {\"keyword\": \"jewish\", \"function\": [\"ADJ\"], \"category\": \"judaism\"},\n","        {\"keyword\": \"jews\", \"function\": [\"PROPN\", \"NOUN\"], \"category\": \"judaism\"},\n","        {\"keyword\": \"judaism\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"judaism\"},\n","        {\n","            \"keyword\": \"muslim\",\n","            \"function\": [\"NOUN\", \"ADJ\"],\n","            \"category\": \"islam\",\n","        },\n","        {\"keyword\": \"muslims\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"islam\"},\n","        {\"keyword\": \"moslem\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"islam\"},\n","        {\"keyword\": \"moslems\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"islam\"},\n","        {\"keyword\": \"islam\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"islam\"},\n","        {\n","            \"keyword\": \"christ\",\n","            \"function\": [\"NOUN\", \"PROPN\"],\n","            \"category\": \"christianity\",\n","        },\n","        {\n","            \"keyword\": \"christian\",\n","            \"function\": [\"NOUN\", \"PROPN\"],\n","            \"category\": \"christianity\",\n","        },\n","        {\n","            \"keyword\": \"christianity\",\n","            \"function\": [\"NOUN\", \"PROPN\"],\n","            \"category\": \"christianity\",\n","        },\n","        {\n","            \"keyword\": \"christians\",\n","            \"function\": [\"NOUN\", \"PROPN\"],\n","            \"category\": \"christianity\",\n","        },\n","        {\"keyword\": \"buddhism\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"buddhism\"},\n","        {\"keyword\": \"buddhist\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"buddhism\"},\n","        {\"keyword\": \"buddhists\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"buddhism\"},\n","        {\"keyword\": \"hindu\", \"function\": [\"NOUN\"], \"category\": \"hinduism\"},\n","        {\"keyword\": \"hindus\", \"function\": [\"NOUN\"], \"category\": \"hinduism\"},\n","        {\"keyword\": \"hinduism\", \"function\": [\"NOUN\", \"PROPN\"], \"category\": \"hinduism\"},\n","    ]'''\n","\n","# how to add new concepts in the same manner\n","\n","names_concept = Concept.from_dict_keyword_list(\n","    name=\"names1\",\n","    lang=\"en\",\n","    keywords=[{\"keyword\": \"Hans\", \"function\": [\"NOUN\"]}],\n",")\n","\n","bias_detector.register_concept(names_concept)\n","\n","# add concept to 'concepts_to_consider' bias_detector.process()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["counterfactual_detection_results = bias_detector.process(\n","    texts=df.comment_text.sample(1000, random_state=42),\n","    labels=df.target.tolist(),\n","    predict_func=clf.predict_proba,\n","    concepts_to_consider=[\"religion\", \"gender\", \"ethnicity\", \"names1\"],\n","    max_counterfactual_samples=None,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# 2.2 Interpretation der Counterfactual Scores\n","\n","Biaslyze identifiziert:\n","\n","- welche Begriffe keinen Einfluss haben (omitted keywords)\n","- welche Begriffe starken Einfluss haben (Counterfactual Score)\n","\n","Ein hoher Score bedeutet: Das Modell reagiert stark auf den Begriff, unabhängig vom restlichen Kontext\n","\n","Für sensible Konzepte gibt dies wertvolle Hinweise:\n","\n","- Welche Begriffe triggern stark?\n","- Welche Begriffe sollten überprüft werden?\n","- Welche verzerrte Gewichtung könnte das Modell gelernt haben?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# omitted keywords\n","print(counterfactual_detection_results.concept_results[3].omitted_keywords)"]},{"cell_type":"markdown","metadata":{},"source":["# 2.3 Plotten der Ergebnisse\n","\n","Der couterfactual score wird definiert als die Differenz zwischen dem vorhergesagten Wahrscheinlichkeitswert für den couterfactual Text und  dem vorhergesagten Wahrscheinlichkeitswert für den Originaltext.\n","    \n","> couterfactual_score = P(toxisch | couterfactual_Text) - P(toxisch | Originaltext)\n","\n","Je weiter also die Punktzahl einer Probe von Null entfernt ist, desto größer ist die Änderung in der Entscheidung des Modells, ob ein Kommentar toxisch oder nicht toxisch ist, wenn er durch dieses Schlüsselwort ersetzt wird. In diesem Fall ist die positive Klasse „toxisch” und die negative Klasse „nicht toxisch”. Wie ihr seht, führt das Ersetzen eines beliebigen anderen geschlechtsspezifischen Schlüsselworts durch das Wort „Mutter” dazu, dass die Klassifizierung der Proben mit größerer Wahrscheinlichkeit als „toxisch” eingestuft wird."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["concept = \"gender\"\n","top_n = 20 \n","\n","\n","### Ignore the following code - only plots the data \n","\n","def _plot_box_plot(dataf: pd.DataFrame, top_n: int = None):\n","    \"\"\"Plot a box plot of scores.\n","\n","    Args:\n","        dataf: A dataframe with scores for each sample.\n","        top_n: Only plot the top n concepts.\n","\n","    Returns:\n","        A matplotlib axis.\n","    \"\"\"\n","    # sort the dataframe by median absolute value\n","    sort_index = dataf.median().abs().sort_values(ascending=True)\n","    sorted_dataf = dataf[sort_index.index]\n","    if top_n:\n","        sorted_dataf = sorted_dataf.iloc[:, -top_n:]\n","    ax = sorted_dataf.plot.box(\n","        vert=False, figsize=(12, int(sorted_dataf.shape[1] / 2.2))\n","    )\n","    ax.vlines(\n","        x=0,\n","        ymin=0.5,\n","        ymax=sorted_dataf.shape[1] + 0.5,\n","        colors=\"black\",\n","        linestyles=\"dashed\",\n","        alpha=0.5,\n","    )\n","    return ax\n","\n","dataf = counterfactual_detection_results._get_result_by_concept(concept=concept)\n","ax = _plot_box_plot(dataf, top_n=top_n)\n","ax.set_title(\n","            f\"Distribution of counterfactual scores for concept '{concept}'\\nsorted by median score\"\n","        )\n","ax.set_xlabel(\n","            \"Counterfactual scores - differences from zero indicate the direction of bias.\"\n","        )\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# only works with jupyter running on a local machine\n","# besides counterfactual score, ksr, sample based histograms and the augmented textsamples can be explored \n","\n","counterfactual_detection_results._get_counterfactual_samples_by_concept(concept=\"religion\")\n","counterfactual_detection_results.dashboard(num_keywords= 10, port = 8090)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1498861,"sourceId":2476686,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.10.18 ('bias_env')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"vscode":{"interpreter":{"hash":"bd8782d55280fa38bb018a54b6a9b60a28993971bca746cefb9a228699595972"}}},"nbformat":4,"nbformat_minor":4}
